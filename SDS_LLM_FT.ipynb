{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/starmpcc/SamsungSDS-LLM-Tutorial-2024/blob/main/SDS_LLM_FT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9cvXkGIWXj4"
      },
      "source": [
        "# Samsung SDS LLM Fine-Tuning Tutorial\n",
        "\n",
        "- 연자: 김준우(kjune0322@kaist.ac.kr), 권순준(sean0042@kaist.ac.kr)\n",
        "- 발표자료: https://github.com/starmpcc/SamsungSDS-LLM-Tutorial-2024\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3qNfAh9IYFx"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/starmpcc/SamsungSDS-LLM-Tutorial-2024/blob/main/SDS_LLM_FT.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dY38frHA5rk_"
      },
      "outputs": [],
      "source": [
        "# First, install required packages\n",
        "!pip install -q accelerate==0.25.0 peft==0.6.2 bitsandbytes==0.41.1 transformers==4.36.2 trl==0.7.4 einops gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cyq7qXBK5hrq"
      },
      "outputs": [],
      "source": [
        "# Import Libraries\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        ")\n",
        "from peft import LoraConfig\n",
        "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
        "import gradio as gr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xbz2wAkMV3TZ"
      },
      "outputs": [],
      "source": [
        "# To save time, first download model and data\n",
        "\n",
        "# Define Quantization Config\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        ")\n",
        "\n",
        "# Load Model and Dataset\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"microsoft/phi-2\",\n",
        "    trust_remote_code=True,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    revision=\"refs/pr/23\"\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('microsoft/phi-2')\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_sight = \"right\"\n",
        "\n",
        "def prompt_shorter_than(samples):\n",
        "    concatenated = [\" \".join([i, j, k]) for i, j, k in zip(samples['note'], samples['question'], samples['answer'])]\n",
        "    return [len(i)<=320 for i in tokenizer(concatenated)['input_ids']]\n",
        "\n",
        "dataset = load_dataset(\"starmpcc/Asclepius-Synthetic-Clinical-Notes\")\n",
        "dataset = dataset.filter(lambda x: [len(i)<1500 for i in x['note']], batched=True)\n",
        "dataset = dataset.filter(prompt_shorter_than, batched=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JJxznOdbVdu6"
      },
      "outputs": [],
      "source": [
        "# Let's pre-process dataset\n",
        "\n",
        "# First of all, we have to check how the dataset is composed\n",
        "print(dataset['train'])\n",
        "dataset['train'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mNL48UB8XubL"
      },
      "outputs": [],
      "source": [
        "# We make this dataset to phi-2 compatible\n",
        "# Phi-2 instruction-answer format: \"Instruct: <prompt>\\nOutput:\"\n",
        "\n",
        "# Make your own prompt!\n",
        "prompt_template=\"\"\"Instruct: Please write down your own prompt.\n",
        "For instance, you can insert the note as {{note}}\n",
        "{note}\n",
        "Model should answer to {{question}} based on the note.\n",
        "{question}\n",
        "You should maintain the phi-2 format\n",
        "Accordingly, the last line must be like the below.\n",
        "Do not forget to insert a new line between your prompt and 'Output'!\n",
        "Output: {answer}\n",
        "\"\"\"\n",
        "\n",
        "# Should get Dict[List] as input, return list of prompts\n",
        "def format_dataset(samples):\n",
        "    outputs = []\n",
        "    for _, note, question, answer, _ in zip(*samples.values()):\n",
        "        out = prompt_template.format(note=note, question=question, answer=answer)\n",
        "        outputs.append(out)\n",
        "    return outputs\n",
        "\n",
        "sample_input = format_dataset({k: [v] for k, v in dataset['train'][0].items()})[0]\n",
        "print(sample_input)\n",
        "print(\"*\"*20)\n",
        "\n",
        "# Sanity Check\n",
        "prompt_len = len(tokenizer.encode(prompt_template))\n",
        "if prompt_len > 180:\n",
        "    raise ValueError(f\"Your prompt is too long! Please reduce the length from {prompt_len} to 180 tokens\")\n",
        "print(f\"Prompt Length: {prompt_len} tokens\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-F0CmI16j_l"
      },
      "outputs": [],
      "source": [
        "sample_idx = 0\n",
        "sample_input = format_dataset({k: [v] for k, v in dataset['train'][sample_idx].items()})[0].split('Output: ')[0]\n",
        "input_ids = tokenizer.encode(sample_input, return_tensors='pt').to('cuda')\n",
        "with torch.no_grad():\n",
        "  output = model.generate(input_ids=input_ids,\n",
        "                            max_length=512,\n",
        "                            use_cache=True,\n",
        "                            temperature=0.,\n",
        "                            eos_token_id=tokenizer.eos_token_id\n",
        "  )\n",
        "print(tokenizer.decode(output.to('cpu')[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bXTfKImbvEC"
      },
      "outputs": [],
      "source": [
        "# Then, let's define dataset.\n",
        "response_template = \"Output:\"\n",
        "collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)\n",
        "\n",
        "train_dataset = dataset['train']\n",
        "sampled_train_dataset = train_dataset.select(range(2000))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y8jY6PNfa8Sm"
      },
      "outputs": [],
      "source": [
        "# SFTTrainer Do everything else for you!\n",
        "\n",
        "lora_config=LoraConfig(\n",
        "    r=4,\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules= [\"Wqkv\", \"fc1\", \"fc2\" ]\n",
        ")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=1,\n",
        "    fp16=True,\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=8,\n",
        "    learning_rate=1e-4,\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    save_strategy=\"no\",\n",
        "    warmup_ratio=0.03,\n",
        "    logging_steps=5,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    report_to=\"tensorboard\",\n",
        "    gradient_checkpointing=True\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model,\n",
        "    training_args,\n",
        "    train_dataset=sampled_train_dataset,\n",
        "    formatting_func=format_dataset,\n",
        "    data_collator=collator,\n",
        "    peft_config=lora_config,\n",
        "    max_seq_length=512,\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "n_Ngq49_BA2E"
      },
      "outputs": [],
      "source": [
        "# Run Training\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_ntjMAe6Uoh"
      },
      "outputs": [],
      "source": [
        "# Wrap-up Training\n",
        "model = trainer.model\n",
        "model.eval()\n",
        "\n",
        "note_samples = train_dataset.select(range(len(train_dataset)-10, len(train_dataset)))['note']\n",
        "\n",
        "def inference(note, question, model):\n",
        "    prompt = prompt_template.format(note=note, question=question, answer=\"\")\n",
        "    tokens = tokenizer.encode(prompt, return_tensors=\"pt\").to('cuda')\n",
        "    outs = model.generate(input_ids=tokens,\n",
        "                          max_length=512,\n",
        "                          use_cache=True,\n",
        "                          temperature=0.,\n",
        "                          eos_token_id=tokenizer.eos_token_id\n",
        "                          )\n",
        "    output_text = tokenizer.decode(outs.to('cpu')[0], skip_special_tokens=True)\n",
        "    return output_text[len(prompt):]\n",
        "\n",
        "\n",
        "def compare_models(note, question):\n",
        "    with torch.no_grad():\n",
        "        asc_answer = inference(note, question, trainer.model)\n",
        "        with model.disable_adapter():\n",
        "            phi_answer = inference(note, question, trainer.model)\n",
        "    return asc_answer, phi_answer\n",
        "\n",
        "demo = gr.Interface(fn=compare_models, inputs=[gr.Dropdown(note_samples), \"text\"], outputs=[gr.Textbox(label=\"Asclepius\"), gr.Textbox(label=\"Phi-2\")])\n",
        "demo.launch()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}